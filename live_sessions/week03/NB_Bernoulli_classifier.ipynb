{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Week 3: The Naive Bayes classifier - Bernoulli model\n",
    "\n",
    "Instructor: Cornelia Ilin <br>\n",
    "Email: cilin@ischool.berkeley.edu <br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Import packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Define working directories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Define classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Define functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Step 5: Read data\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/'\n",
    "                      'ml/machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "\n",
    "df.columns = ['class_label', 'alcohol', 'malic_acid', 'ash',\n",
    "              'alcalinity_of_ash', 'magnesium', 'total_pphenols',\n",
    "              'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins',\n",
    "              'color_intensity', 'hue', 'OD280/OD315_of_diluted_wines',\n",
    "              'proline']\n",
    "\n",
    "print('Shape of df wine:', df.shape)\n",
    "print('Class labels:', df['class_label'].unique())\n",
    "print()\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Step 6: Data preprocessing (a.k.a., data cleaning)\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 6.1 Data subseting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Today we will only be working with all class labels but only four features ['acohol', 'malic_acid', 'ash', 'flavanoids']."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "labels = ['class_label']\n",
    "features = ['alcohol', 'malic_acid', 'ash', 'color_intensity']\n",
    "df = df[labels+features]\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create X and y arrays\n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the feature values are real valued. Let's look at histograms of each features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a new figure and set the figsize argument so we get square-ish plots of the 4 features.\n",
    "plt.figure(figsize=(15, 3))\n",
    "\n",
    "# Iterate over the features, creating a subplot with a histogram for each one.\n",
    "for idx, feature in enumerate(features):\n",
    "    plt.subplot(1, 4, idx+1)\n",
    "    X[feature].hist(bins=10, grid=False)\n",
    "    plt.title(feature)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 6.2 Binarize feature values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make things simple, let's binarize these feature values. That is, we'll treat each measurement as either \"low\" or \"high\". I'm just going to choose a threshold for each feature."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for feature in features:\n",
    "    X[feature] = np.where(X[feature] >= X[feature].mean(), 1, 0)\n",
    "X.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 6.3 Split data into train and test sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
    "                                                    random_state=1, stratify=y)\n",
    "\n",
    "train_df = pd.concat((y_train, X_train), axis=1)\n",
    "train_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Class labels:', train_df.class_label.unique())\n",
    "train_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 6.4 Compute stats needed for Bayes' probabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Step 6.4.1 Compute feature = 0 frequency (apply count() by column)\n",
    "\n",
    "needed to compute conditional probabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute feature = 0 frequency  \n",
    "feature0_count = pd.DataFrame()\n",
    "\n",
    "for feature in features:\n",
    "    temp_train_df = train_df[['class_label', feature]]\n",
    "    temp_train_df = temp_train_df[train_df[feature]==0]\n",
    "    temp_count = temp_train_df.groupby(['class_label']).count()\n",
    "    feature0_count = pd.concat([feature0_count, temp_count], axis=1)\n",
    "\n",
    "feature0_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Step 6.4.2 Compute feature = 1 frequency (apply count() by column)\n",
    "\n",
    "Note that it sufficies to compute only feature = 0 frequency. Why?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute feature = 1 frequency\n",
    "feature1_count = pd.DataFrame()\n",
    "\n",
    "for feature in features:\n",
    "    temp_train_df = train_df[['class_label', feature]]\n",
    "    temp_train_df = temp_train_df[train_df[feature]==1]\n",
    "    temp_count = temp_train_df.groupby(['class_label']).count()\n",
    "    feature1_count = pd.concat([feature1_count, temp_count], axis=1)\n",
    "\n",
    "feature1_count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# As a sanity check/, what should the total sum of all counts (feature0_count + feature1_count) be?\n",
    "# We have 133 training examples, each with 4 features. So we should have counted 532 things.\n",
    "print(feature0_count.sum() + feature1_count.sum())\n",
    "\n",
    "print('\\ntrain_df.shape: ', train_df.iloc[:, 1:].shape)\n",
    "print('Total count: ', train_df.iloc[:,1:].shape[0] * train_df.iloc[:,1:].shape[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 6.4.5 compute number of y_train, y_train=1, y_train=2, y_train=3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count_y_train_all = train_df['class_label'].count()\n",
    "count_y_train_1 = train_df[train_df.class_label==1]['class_label'].count()\n",
    "count_y_train_2 = train_df[train_df.class_label==2]['class_label'].count()\n",
    "count_y_train_3 = train_df[train_df.class_label==3]['class_label'].count()\n",
    "count_y_train = pd.Series(np.array([count_y_train_1, count_y_train_2, count_y_train_3]), index= [1,2,3])\n",
    "\n",
    "print('count_y_train_all: ', count_y_train_all)\n",
    "print('count_y_train_1: ', count_y_train_1)\n",
    "print('count_y_train_2: ', count_y_train_2)\n",
    "print('count_y_train_3: ', count_y_train_3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### Step 7: Analysis - Naive Bayes as a classification algorithm\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 7.1 Compute conditional probabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute conditional probability for each feature=0\n",
    "cond_prob0 = feature0_count.apply(lambda x: x/count_y_train)\n",
    "cond_prob0\n",
    "\n",
    "# add conditional probability for all feature=0\n",
    "cond_prob0['cond_prob0'] = cond_prob0.apply(lambda x: x.alcohol * x.malic_acid * x.ash * x.color_intensity, axis=1)\n",
    "cond_prob0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute conditional probability for each feature=1\n",
    "cond_prob1 = feature1_count.apply(lambda x: x/count_y_train)\n",
    "cond_prob1\n",
    "\n",
    "# add conditional probability for all feature=1\n",
    "cond_prob1['cond_prob1'] = cond_prob1.apply(lambda x: x.alcohol * x.malic_acid * x.ash * x.color_intensity, axis=1)\n",
    "cond_prob1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# As a sanity check, which probabilities should sum to 1?\n",
    "sum_cond_prob = cond_prob1 + cond_prob0\n",
    "sum_cond_prob[features]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 7.2 Compute prior probabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prior_1 = count_y_train_1/count_y_train_all\n",
    "prior_2 = count_y_train_2/count_y_train_all\n",
    "prior_3 = count_y_train_3/count_y_train_all\n",
    "priors = pd.Series(np.array([prior_1, prior_2, prior_3]), index= [1,2,3])\n",
    "\n",
    "print('P(class=1)=', prior_1)\n",
    "print('P(class=2)=', prior_2)\n",
    "print('P(class=3)=', prior_3)\n",
    "\n",
    "# check to see if they add up to 1\n",
    "print('sum prior probabilities:', prior_1 + prior_2 + prior_3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 7.3 Make a prediction for the first example\n",
    "\n",
    "Now that we have all the pieces, let's try making a prediction for the first test example. It looks like this is a cultivar 1 (class_label=1) example. Alcohol and malic acid features have high values, and ash and color intensity features have low values.\n",
    "\n",
    "We start by assuming the prior distribution, which has a slight preference for cultivar 2, followed by cultivar 1. Of course, these estimates come from our training data, which might not be a representative sample. In practice, we may prefer to use a uniform prior."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# What does the feature vector look like? And what's the true label?\n",
    "index = 0\n",
    "print ('Feature vector: \\n', X_test.iloc[index, :])\n",
    "print ('\\nClass label: ', y_test[index])\n",
    "\n",
    "# Start with the prior distribution over labels.\n",
    "print ('\\nPriors:\\n', priors)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's find the predicted label for this test example by accounting for the **alcohol feature** only."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# just as a reminder, print conditional probabilities for feature=1\n",
    "cond_prob1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(a) Compute conditional probabilities * prior probabilities for each class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numerator = []\n",
    "\n",
    "for class_label in range(len(priors)):\n",
    "    numerator.append(cond_prob1['alcohol'].values[class_label] * priors.values[class_label])\n",
    "\n",
    "print(numerator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) compute evidence"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "denominator = 0\n",
    "for val in numerator:\n",
    "    denominator += val\n",
    "\n",
    "denominator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "compute posterior probability"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "posterior_prob = list(numerator/denominator)\n",
    "posterior_prob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So after accounting for the alcohol feature our updated belief is that the train obsetvation is part of cultivar 1.\n",
    "\n",
    "Let's include the remaining features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute conditional probabilities * prior probabilities\n",
    "numerator = []\n",
    "\n",
    "for class_label in range(len(priors)):\n",
    "    numerator.append(cond_prob1['alcohol'].values[class_label] * cond_prob1['malic_acid'].values[class_label] *\n",
    "                     priors.values[class_label])\n",
    "    \n",
    "numerator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute evidence\n",
    "denominator = 0\n",
    "for val in numerator:\n",
    "    denominator += val\n",
    "\n",
    "denominator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compute posterior probability\n",
    "posterior_prob = list(numerator/denominator)\n",
    "posterior_prob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So after accounting for all the feature our updated belief is that the train obsetvation is part of cultivar 3. It looks like Naive Bayes came up with the wrong answer."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 7.4 Make a prediction using sklearn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can compare our implementation with the sklearn implementation. Do the predictions agree? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred = bnb.predict(X_test)\n",
    "print('Predicted label for the first test example is: ', y_pred[0])\n",
    "y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question: what's the role of alpha here?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's compute accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Prediction accuracy: %3.2f' % bnb.score(X_test, y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's compare our prior probabilities with sklearn's"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print ('\\nour prior probabilities:\\n', list(priors))\n",
    "print ('\\nsklearn prior probabilities:\\n', np.exp(bnb.class_log_prior_))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's compare our conditional probabilities for feature=1 with sklearn's"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print ('\\nOur conditional probabilities\\n', cond_prob1.loc[:,features])\n",
    "print ('\\nsklearn conditional probabilities\\n', np.exp(bnb.feature_log_prob_))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "ac790038bef2eac1a9d9ccab13fd5e8972b5c1c9ef161fffafb9eb23a97c981f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}