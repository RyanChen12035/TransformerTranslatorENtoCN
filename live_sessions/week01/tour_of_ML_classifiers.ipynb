{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Week 1: Tour of ML classifiers\n",
    "\n",
    "Instructor: Nedelina Teneva <br>\n",
    "Email: nteneva@berkeley.edu <br>\n",
    "\n",
    "\n",
    "Citations: \n",
    "  - Chapter 3, Python Machine Learning 3rd Edition by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2019\n",
    "  - Cornelia Ilin's prep material: https://github.com/MIDS-W207/cilin-coursework/tree/master/live_sessions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Objective: \n",
    " - take a tour of some popular and powerful ML algorithms for supervised classification:\n",
    "     -- perceptron\n",
    "     -- logistic regression\n",
    "     -- support vector machines\n",
    "     -- Naive Bayes\n",
    "### Breakout room task: \n",
    "- repeat Steps 9-25 using the 'ex2data2.txt' file you can find in live_sessions/week1/data folder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Import packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# general\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plots\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# predictions\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Set working directories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Define classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Perceptron_self(object):\n",
    "    \"\"\" Perceptron classifier\n",
    "    # param eta: float, learning rate (between 0.0 and 1.0)\n",
    "    # param n_iter: int, passes over the training dataset\n",
    "    # param random_state: int, random number generator seed for random weight initialization\n",
    "    \n",
    "    # attribute w_: 1d-array, weights after fitting\n",
    "    # attribute errors_: list, number of misclasifications (updates) in each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit training data\n",
    "        # param X:array-like, shape = [n_examples, n_features]\n",
    "        # param y: array_like, shape =[n_examples]\n",
    "        # return self: object\n",
    "        \"\"\"\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Define functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_decision_regions(X, y, classifier, plot_test, resolution=0.02):\n",
    "    \"\"\" Plot decission regions, perceptron model\n",
    "    # param X: <<your task to comment here>>\n",
    "    # param y: <<your task to comment here>>\n",
    "    # param classifier: <<your task to comment here>>\n",
    "    # param plot_test: integer, 1 if plot test sample, 0 otherwise\n",
    "    # param resolution: <<your task to comment here>>\n",
    "    # return: None\n",
    "    \"\"\"\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')\n",
    "        \n",
    "    # highlight text examples    \n",
    "    if plot_test == 1:\n",
    "        test_idx = range(105, 150)\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0],\n",
    "                    X_test[:, 1],\n",
    "                    c='',\n",
    "                    edgecolor='black',\n",
    "                    alpha=1.0,\n",
    "                    linewidth=1,\n",
    "                    marker='o',\n",
    "                    s=100, \n",
    "                    label='test set')\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Classification on the Iris dataset with a user-defined Class for the Perceptron model\n",
    "___"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5: Read data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s = os.path.join('https://archive.ics.uci.edu', 'ml',\n",
    "                 'machine-learning-databases', 'iris','iris.data').replace('\\\\', '//')\n",
    "print('URL:', s)\n",
    "\n",
    "df = pd.read_csv(s,\n",
    "                 header=None,\n",
    "                 encoding='utf-8')\n",
    "print('\\nShape of data:', df.shape)\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print type of flowers\n",
    "df[4].unique()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Image(filename='./images/iris.PNG', width = 400)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 6: Plot data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "we will consider only two flower classes (Setosa and Versicolor) for practical reasons.\n",
    "\n",
    "we will also restrict the analysis to only two feature variables, sepal length and petal length (easier to visualize the decission boundary in a 2D space)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# select setosa and versicolor\n",
    "y = df.iloc[0:100, 4].values\n",
    "y = np.where(y == 'Iris-setosa', -1, 1)\n",
    "\n",
    "# extract sepal length and petal length\n",
    "X = df.iloc[0:100, [0, 2]].values\n",
    "\n",
    "# plot X data for setosa\n",
    "plt.scatter(X[:50, 0], X[:50, 1],\n",
    "            color='red', marker='o', label='setosa')\n",
    "\n",
    "# plot X data for versicolor\n",
    "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
    "            color='blue', marker='x', label='versicolor')\n",
    "\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# plt.savefig('images/02_06.png', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 7: Training the perceptron model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question: can we use the perceptron to classify the two flowers in this dataset? <br>\n",
    "The perceptron is a linear classifier, and in this example we can use a linear decission boundary to separate Setosa from Versicolor flowers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppn = Perceptron_self(eta=0.1, n_iter=10)\n",
    "\n",
    "ppn.fit(X, y)\n",
    "\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of missclassification errors')\n",
    "\n",
    "# plt.savefig('images/02_07.png', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question: what can we conclude from this graph? <br>\n",
    "Answer: that our perceptron converged after 6 iterrations (epochs) and should now be able to classify the training sample perfectly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 8: Visualize the decission boundaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_decision_regions(X, y, classifier=ppn, plot_test=0)\n",
    "plt.xlabel('sepal length [cm]')\n",
    "plt.ylabel('petal length [cm]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Classification on the Iris dataset with scikit-learn\n",
    "___"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 9: Read data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# loading the Iris dataset from scikit-learn\n",
    "iris = datasets.load_iris()\n",
    "pd.concat((pd.DataFrame(iris.data), pd.DataFrame(iris.target)), 1).head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use only two features, the sepal length and petal length.\n",
    "\n",
    "We will now work with 3 classes; these are converted to integer labels: 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target \n",
    "print('Class labels: ', np.unique(y))\n",
    "print('Length of data:', len(X))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question: Can you work with class labels in string format?<br>\n",
    "Answer: Yes, but working with integer labels is more computationally effective (smaller memory use). Encoding class labels is a common convention among most ML libraries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 10: Split data into training and test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "this is needed to evaluate how well the model performs on unseen data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=1, stratify=y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the train_test_split() already shuffles the training datasets (we want a mix of all classes both in the training and test set). <br>\n",
    "the stratify parameter creates training and test datasets that have the same proportions of class labels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 11: Feature scalling for optimal performance\n",
    "\n",
    "We will use the StandardScaler class from scikit-learn's preprocessing module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sc = StandardScaler()\n",
    "# estimate the sample mean and standard deviation for each feature in X_train\n",
    "sc.fit(X_train)\n",
    "\n",
    "# use the two parameters to standardize both X_train and X_test\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Train a perceptron model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 12: Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "the scikit-learn library supports multiclass classification via the **one-vs.-rest(OvR)** method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ppn = Perceptron(eta0=0.1, random_state=1)\n",
    "ppn.fit(X_train_std, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 13: Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Length y_pred: ', len(y_pred))\n",
    "y_pred\n",
    "\n",
    "print('Misclassified examples: %d (out of 45)' % (y_test != y_pred).sum())\n",
    "error = (y_test != y_pred).sum()/len(y_pred)\n",
    "print('Misclassification error: %.3f' % error)\n",
    "print('Accuracy: %.3f'% (1-error))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 13: Visualize the decission boundaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to visualize how well the perceptron separates the 3 different flowers (note that this time we plot the test data as well)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine training and test data\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n",
    "plot_decision_regions(X=X_combined_std, y=y_combined, classifier=ppn, plot_test=1)\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can conclude from this plot that the 3 flower classes cannot be perfectly separated by a linear decission boundary (main assumption of the perceptron)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's turn our attention to more powerful classifiers. Note that we will be recyclying the data generated at Step 9-11 for the next examples."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Train a logistic regression model "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model is easy to implement and performs well on linearly separable classes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 14: Train "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lr = LogisticRegression(C=100.0, random_state=1, solver='lbfgs', multi_class='multinomial')\n",
    "lr.fit(X_train_std, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 15: Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = lr.predict(X_test_std)\n",
    "print('Length y_pred: ', len(y_pred))\n",
    "y_pred\n",
    "\n",
    "print('Misclassified examples: %d (out of 45)' % (y_test != y_pred).sum())\n",
    "error = (y_test != y_pred).sum()/len(y_pred)\n",
    "print('Misclassification error: %.3f' % error)\n",
    "print('Accuracy: %.3f'% (1-error))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 16: Visualize the decission boundaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Logistic regression seems to be doing a better job at separating the 3 types of flowers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_decision_regions(X_combined_std, y_combined,\n",
    "                      classifier=lr, plot_test=1)\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Train a (linear) SVM model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model performs well on linearly separable classes. Linear logistic regression and linear SVM often yield very similar results. \n",
    "\n",
    "Logistic regression tries to maximize the conditional likelihood of the training data. so this means that it pays equal attention to outliers.\n",
    "\n",
    "SVM avoids this problem, because it cares mostly about the points that are closest to the decission boundary (support vectors)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 17: Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svm = SVC(kernel='linear', random_state=1, C=1.0)\n",
    "svm.fit(X_train_std, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 18: Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = svm.predict(X_test_std)\n",
    "print('Length y_pred: ', len(y_pred))\n",
    "y_pred\n",
    "\n",
    "print('Misclassified examples: %d (out of 45)' % (y_test != y_pred).sum())\n",
    "error = (y_test != y_pred).sum()/len(y_pred)\n",
    "print('Misclassification error: %.3f' % error)\n",
    "print('Accuracy: %.3f'% (1-error))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 19: Visualize the decission boundaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_decision_regions(X_combined_std, y_combined, \n",
    "                      classifier=svm, plot_test=1)\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/03_16.png', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Train a (nonlinear) SVM model: kernel SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SVM can be easily **kernalized** to solve nonlinear classification problems. \n",
    "\n",
    "The idea is to create nonlinear combinations of the original features, and then project them onto a higher-dimensional space via a mapping function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 20: Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question: what happens if we change the value of gamma? Let's try 100.0 (gamma increases the influence of the training examples)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svm = SVC(kernel='rbf', random_state=1, gamma=100.0, C=1.0)\n",
    "svm.fit(X_train_std, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 21: Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = svm.predict(X_test_std)\n",
    "print('Length y_pred: ', len(y_pred))\n",
    "y_pred\n",
    "\n",
    "print('Misclassified examples: %d (out of 45)' % (y_test != y_pred).sum())\n",
    "error = (y_test != y_pred).sum()/len(y_pred)\n",
    "print('Misclassification error: %.3f' % error)\n",
    "print('Accuracy: %.3f'% (1-error))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 22: Visualize the decission boundaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_decision_regions(X_combined_std, y_combined, \n",
    "                      classifier=svm, plot_test=1)\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/03_16.png', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___\n",
    "### Train a Naive Bayes model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 23: Train"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train_std, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 24: Predict"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred = nb.predict(X_test_std)\n",
    "print('Length y_pred: ', len(y_pred))\n",
    "y_pred\n",
    "\n",
    "print('Misclassified examples: %d (out of 45)' % (y_test != y_pred).sum())\n",
    "error = (y_test != y_pred).sum()/len(y_pred)\n",
    "print('Misclassification error: %.3f' % error)\n",
    "print('Accuracy: %.3f'% (1-error))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 25: Visualize the decission boundaries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_decision_regions(X_combined_std, y_combined, \n",
    "                      classifier=nb, plot_test=1)\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('images/03_16.png', dpi=300)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "ac790038bef2eac1a9d9ccab13fd5e8972b5c1c9ef161fffafb9eb23a97c981f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}