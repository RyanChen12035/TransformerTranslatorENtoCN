{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression (1 feature)\n",
        "\n",
        "Resources:\n",
        "\n",
        "* https://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L02%20Linear%20Regression.pdf\n"
      ],
      "metadata": {
        "id": "qzfPOmyKvduy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imcNmFXhPdCh"
      },
      "source": [
        "# Import our standard libraries.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style='darkgrid')  # default style\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VloE32t7dkU1"
      },
      "source": [
        "## Data for Linear Regression\n",
        "\n",
        "Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$. Each datapoint represents the value of a single feature. Because we'll want our model to have a learned *bias* (or *intercept*), we will use an extra feature which will always be set to 1.\n",
        "\n",
        "E.g. take y = ax + b -> a = slope of line, b = (y) intercept of line.\n",
        "\n",
        "y = ax + b*1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bcduWsAbCRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be516f65-9535-42e3-d953-761db22ba421"
      },
      "source": [
        "# Here are our inputs.\n",
        "X = np.array([[1, 3],\n",
        "              [1, 2]])\n",
        "Y = np.array([7, 5])\n",
        "print (X)\n",
        "print (Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 3]\n",
            " [1 2]]\n",
            "[7 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSWDARd9eM99"
      },
      "source": [
        "## Example slicing\n",
        "Practice slicing the input array X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQiqqBTd-t3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975c644f-d840-4374-b9d1-ff3436539d2c"
      },
      "source": [
        "# Use slicing to get the first X example (as an array)\n",
        "x_e0 = X[0,:]\n",
        "# Use slicing to get the second X example (as an array)\n",
        "x_e1 = X[1,:]\n",
        "print (x_e0)\n",
        "print (x_e1)\n",
        "\n",
        "print (X[:, 1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 3]\n",
            "[1 2]\n",
            "[3 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsuG8A-zeUV6"
      },
      "source": [
        "## Make predictions\n",
        "Suppose we are using linear regression. Then the functional form for our model is: \n",
        "\n",
        "\\begin{align}\n",
        "h(x) &= w_0 + w_1x \n",
        "\\end{align}\n",
        "\n",
        "Since we're using an extra feature corresponding to the intercept/bias, we can write:\n",
        "\n",
        "\\begin{align}\n",
        "h(x) &= w_0x_0 + w_1x_1 \\\\\n",
        "\\end{align}\n",
        "\n",
        "Given parameter values, practice computing model predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDbc2mGXcTvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8232be3f-7b70-402c-b1e9-b4fbc0919676"
      },
      "source": [
        "# Let's use a linear regression model: f(x) = w0 + w1*x1\n",
        "W = np.array([100, 100])\n",
        "\n",
        "# Compute the prediction of the model for the first X example\n",
        "pred_e0 = np.dot(W, X[0])\n",
        "print (W)\n",
        "print (X[0])\n",
        "print (X[1])\n",
        "print(pred_e0)\n",
        "\n",
        "# Compute the prediction of the model for the second X example\n",
        "pred_e1 = np.dot(W, X[1])\n",
        "\n",
        "print(pred_e1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100 100]\n",
            "[1 3]\n",
            "[1 2]\n",
            "400\n",
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdGfEoDovBm"
      },
      "source": [
        "Let's rewrite our model function with matrix multiplication and using the notation $h_W$ to make clear that our model is *parameterized* by $W$ (the vector of parameters):\n",
        "\n",
        "\\begin{align}\n",
        "h_W(x) = w_0x_0 + w_1x_1 = xW^T\n",
        "\\end{align}\n",
        "\n",
        "To make this matrix formulation as clear as possible, this is:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y} = h_W(x) = xW^T =\n",
        "\\begin{pmatrix}\n",
        "x_0 & x_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "In addition, if we wanted to apply our model to *all* inputs $X$, we could simply use $XW^T$:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{Y} = h_W(X) = XW^T =\n",
        "\\begin{pmatrix}\n",
        "x_{0,0} & x_{0,1} \\\\\n",
        "x_{1,0} & x_{1,1} \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "x_{m-1,0} & x_{m-1,1} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Remember that [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) requires the inner dimensions to line up: \n",
        "\n",
        "\\begin{align}\n",
        "X_{\\{m \\times n\\}} W^T_{\\{n \\times 1 \\}}  = \\hat{Y}_{\\{m \\times 1 \\}}\n",
        "\\end{align}\n",
        "\n",
        "**Dimensions** --> X: m x 2; W: 1x 2 \n",
        "\n",
        "Use numpy functions to compute predictions for both X examples at once. The result should be a vector with 2 entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRyOr1S1oWAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9bc376d-da1f-4e85-9ee1-8fe698b5923e"
      },
      "source": [
        "# Compute predictions for all X examples at once.\n",
        "#X: 2x2\n",
        "#W.T: 2x1 \n",
        "#Y_hat: 2x1\n",
        "preds = np.dot(X, W.T)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[400 300]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8ZlvnYOpWxR"
      },
      "source": [
        "## Loss\n",
        "Use numpy functions to compute a vector of differences between model predictions and labels (values of Y)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UEd8aEbokKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0811f9-c712-4561-8bde-5f93190111fc"
      },
      "source": [
        "# Compute differences between predictions and labels.\n",
        "diff = preds - Y\n",
        "print(diff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[393 295]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J90USA7GqA2b"
      },
      "source": [
        "Now compute the MSE loss. The result should be a single (scalar) value. Remember we're using this formula (see assignment 1):\n",
        "\n",
        "\\begin{align}\n",
        "J(W) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (h_W(x^{(i)}) - y^{(i)})^2\n",
        "\\end{align}\n",
        "\n",
        "where we've changed the standard scaling from $\\frac{1}{m}$ to $\\frac{1}{2m}$, where $m$ is the number of examples (2, in our case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcjG93R6qe-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45af5566-d1b8-4db1-9911-6bc6701dfcef"
      },
      "source": [
        "# Get the number of examples\n",
        "m = X.shape[0]\n",
        "print (m)\n",
        "# Compute the average per-example loss\n",
        "loss = np.sum(diff**2) / 2*m\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "241474.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWtjfoCZ5n1f"
      },
      "source": [
        "## Gradient\n",
        "\n",
        "Refer to assignment 1 or the gradient descent lecture for the derivation, but here's the formula for the partial derivatives (which together form the gradient):\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial w_j} J(W) &= (h_W(x) - y)x_j\n",
        "\\end{align}\n",
        "\n",
        "This formula is assuming we only have a single example. The general formula has an average over examples:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial w_j} J(W) &= \\frac{1}{m}\\sum_i(h_W(x^{(i)}) - y^{(i)})x^{(i)}_j\n",
        "\\end{align}\n",
        "\n",
        "You're ready to compute the gradient. The result will be a vector of partial derivatives for $w_0$ and $w_1$ respectively. You can use matrix computations as before.\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)X\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwxFEpUr5q9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cafc1efd-fd57-45c6-a522-6371c1698167"
      },
      "source": [
        "# compute the gradient\n",
        "gradient = np.dot(diff, X) / m\n",
        "print(gradient)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[344.  884.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpBEnP5sLZks"
      },
      "source": [
        "## Parameter updates\n",
        "Now that you've computed the gradient, apply parameter updates by subtracting the appropriate partial derivatives (scaled by a learning rate) from the initial parameter values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dtpDEhYZZwg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae8247a4-dd24-4fcc-a284-22f0b9fe288b"
      },
      "source": [
        "# Update parameter values\n",
        "learning_rate = 0.1\n",
        "W = W - learning_rate*gradient\n",
        "print(W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[65.6  11.55]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression (multiple features)\n",
        "\n",
        "Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$, but now we have multiple features."
      ],
      "metadata": {
        "id": "ff3KjlpLvsbG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVGGtopiv39T"
      },
      "source": [
        "# Here are our inputs\n",
        "X = np.array([[1, 3, 1, 1],\n",
        "              [1, 2, 2, 0]])\n",
        "Y = np.array([2, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4bG4AeHv39T"
      },
      "source": [
        "Let's write out our model function:\n",
        "\n",
        "\\begin{align}\n",
        "h_W(x) = w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3 = xW^T\n",
        "\\end{align}\n",
        "\n",
        "We can get all predictions with this matrix product:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{Y} = h_W(X) = XW^T =\n",
        "\\begin{pmatrix}\n",
        "x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3} \\\\\n",
        "x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "w_2 \\\\\n",
        "w_3 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Given the (initial) parameter values below, compute the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGg1Ll4I4jR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e573be-5e1e-4257-dea9-16b46d73258d"
      },
      "source": [
        "# Initial parameter values.\n",
        "W = [1, 1, 1, 1]\n",
        "\n",
        "# Compute predictions\n",
        "preds = np.dot(X, W)\n",
        "print(preds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ_pD-j2_qzV"
      },
      "source": [
        "Now compute the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Sam58B8B1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652291bd-c78c-4231-815d-af1c76b2161d"
      },
      "source": [
        "m, n = X.shape\n",
        "gradient = np.dot((preds - Y), X) / m\n",
        "print(gradient)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4. 10.  6.  2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmYUmEjg_sk8"
      },
      "source": [
        "And now put everything together in gradient descent. You can run multiple steps of gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl_Nu_wB8ar4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728214f7-b2c4-4114-bc5c-de4e30a4ba8b"
      },
      "source": [
        "# Run gradient descent\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# repeat these steps multiple times for multiple epochs -- write your own for loop.\n",
        "preds = np.dot(X, W)\n",
        "loss = ((preds - Y)**2).mean()\n",
        "gradient = 2 * np.dot((preds - Y), X) / m\n",
        "W = W - learning_rate * gradient\n",
        "\n",
        "print('predictions:', preds)\n",
        "print('loss:', loss)\n",
        "print('gradient:', gradient)\n",
        "print('weights:', W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions: [3.0816266  1.91924116]\n",
            "loss: 1.0074602067650509\n",
            "gradient: [2.00086776 5.08336212 2.92010892 1.0816266 ]\n",
            "weights: [ 0.45760669 -0.13882081 -0.03075245  0.94596582]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMsSJ-ZD_12e"
      },
      "source": [
        "## Now with TensorFlow/Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jisaFtGY__KL"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=1,                     # output dim\n",
        "    input_shape=[4],             # input dim\n",
        "    use_bias=False,              # we included the bias in X\n",
        "    kernel_initializer=tf.ones_initializer,  # initialize params to 1\n",
        "))\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "model.compile(loss='mse', optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PQ-RDwXCKVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6298f3ca-c141-4e6b-c41d-5ad4228208c8"
      },
      "source": [
        "history = model.fit(\n",
        "  x = X,\n",
        "  y = Y,\n",
        "  epochs=50,\n",
        "  batch_size=2,\n",
        "  verbose=1)\n",
        "\n",
        "loss = history.history['loss'][0]\n",
        "\n",
        "weights = model.layers[0].get_weights()[0].T\n",
        "\n",
        "preds = model.predict(X)\n",
        "\n",
        "print('predictions:', preds.T)\n",
        "print('loss:', loss)\n",
        "print('W:', weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 0s 247ms/step - loss: 16.0000\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 14.8000\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 13.6936\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 12.6726\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.7297\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 10.8585\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.0530\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.3082\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.6191\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.9815\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.3914\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8452\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.3396\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.8714\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.4379\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.0365\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.6648\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3205\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0017\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7065\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.4330\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.1798\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.9452\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.7279\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.5267\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3403\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.1677\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.0078\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.8597\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.7225\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.5955\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.4778\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3688\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.2678\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.1743\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0877\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0075\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.9332\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.8643\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.8006\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7415\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6868\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.6362\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5892\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5458\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.5055\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4682\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.4337\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4017\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3721\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "predictions: [[2.6332982 1.5368574]]\n",
            "loss: 16.0\n",
            "W: [[0.6152064  0.26255956 0.19826588 1.032147  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we are not performing any evaluation or hyper parameter optimization or tuning in this example. For those two we need a test/validation dataset split."
      ],
      "metadata": {
        "id": "yr3lvoQNu71U"
      }
    }
  ]
}