{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression Demo \n",
        "\n",
        "Instructor: Nedelina Teneva"
      ],
      "metadata": {
        "id": "tCbMTOvHchPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import our standard libraries.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style='darkgrid')  # default style\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Yg0KOj3DcjD6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VloE32t7dkU1"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$, each with 3 features (and a dummy 1 for learning the bias). Now our target labels are binary (0 or 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8bcduWsAbCRl"
      },
      "outputs": [],
      "source": [
        "# Here are our inputs.\n",
        "X = np.array([[1, 3, -2, 0],\n",
        "              [1, 1, 0, 1]])\n",
        "Y = np.array([0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tdGfEoDovBm"
      },
      "source": [
        "Let's write out our model function:\n",
        "\n",
        "\\begin{align}\n",
        "h_W(x) = \\phi(w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3) = \\phi(xW^T) = \\frac{1}{1+e^{(-xW^T)}}\n",
        "\\end{align}\n",
        "\n",
        "We can get all predictions with this matrix product:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{Y} = h_W(X) = \\phi(XW^T)= \\phi\n",
        "\\begin{pmatrix}\n",
        "x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3}\\\\\n",
        "x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3}\\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "w_2 \\\\\n",
        "w_3 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "First let's write the sigmoid (logistic) function $\\phi$. \n",
        "\n",
        "Sigmoid details: https://mathworld.wolfram.com/SigmoidFunction.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hpah13BcCVXo"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLh6VWUfGm7_"
      },
      "source": [
        "Now, given some initial parameter values (below), compute the model's initial predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pGg1Ll4I4jR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6bfede5-49ad-41e7-84ba-64cfaf091ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.88079708 0.95257413]\n"
          ]
        }
      ],
      "source": [
        "# Initial parameter values.\n",
        "W = [1, 1, 1, 1]\n",
        "\n",
        "# Compute predictions.\n",
        "preds = sigmoid(np.dot(X, W))\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIbpXB4ZHPvO"
      },
      "source": [
        "We're not going to use MSE for logistic regression. Instead, we'll use the *logistic loss*, also called *binary cross-entropy* (KL divergence between empirical and predicted distribution).  \n",
        "\n",
        "\\begin{align}\n",
        "LogLoss = \\frac{1}{m} \\sum_i -y_i\\log(\\hat{y_i}) - (1-y_i)\\log(1-\\hat{y_i})\n",
        "\\end{align}\n",
        "\n",
        "Despite this new loss function, it turns out that the gradient computation is the same as it was for MSE with linear regression  (a happy coincidence ...) \n",
        "\\begin{align}\n",
        "\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)X\n",
        "\\end{align}\n",
        "\n",
        "Let's write the code for a single gradient descent step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zl_Nu_wB8ar4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deee90b5-c832-4c02-a231-49af2a5f9002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions: [0.88079708 0.95257413]\n",
            "loss: 1.0877576813083567\n",
            "gradient: [ 0.4166856   1.29748268 -0.88079708 -0.02371294]\n",
            "weights: [0.95833144 0.87025173 1.08807971 1.00237129]\n"
          ]
        }
      ],
      "source": [
        "# Run gradient descent\n",
        "m, n = X.shape  # m = number of examples; n = number of features (including bias)\n",
        "learning_rate = 0.1\n",
        "\n",
        "preds = sigmoid(np.dot(X, W))\n",
        "\n",
        "loss = (-Y * np.log(preds) - (1 - Y) * np.log(1 - preds)).mean()\n",
        "\n",
        "gradient = np.dot((preds - Y), X) / m\n",
        "W = W - learning_rate * gradient\n",
        "\n",
        "print('predictions:', preds)\n",
        "print('loss:', loss)\n",
        "print('gradient:', gradient)\n",
        "print('weights:', W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMsSJ-ZD_12e"
      },
      "source": [
        "## Now with TensorFlow/Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jisaFtGY__KL"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=1,                     # output dim\n",
        "    input_shape=[4],             # input dim\n",
        "    use_bias=False,              # we included the bias in X\n",
        "    activation='sigmoid',        # apply a sigmoid to the output\n",
        "    kernel_initializer=tf.ones_initializer,  # initialize params to 1\n",
        "))\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3PQ-RDwXCKVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9b612a-09a0-4aef-c4c4-7d6421599f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 296ms/step\n",
            "train predictions: [[0.8807971  0.95257413]]\n",
            "loss: 1.0877577066421509\n",
            "W: [[0.8144151  0.01065879 1.8037566  1.2162935 ]]\n"
          ]
        }
      ],
      "source": [
        "# As above, get predictions for the current model first.\n",
        "preds = model.predict(X)\n",
        "\n",
        "# Do gradient descent \n",
        "history = model.fit(\n",
        "  x = X,\n",
        "  y = Y,\n",
        "  epochs=40,\n",
        "  batch_size=2,\n",
        "  verbose=0)\n",
        "\n",
        "# Show the loss (before the update) and the new weights.\n",
        "loss = history.history['loss'][0]\n",
        "weights = model.layers[0].get_weights()[0].T\n",
        "print('train predictions:', preds.T)\n",
        "print('loss:', loss)\n",
        "print('W:', weights)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}