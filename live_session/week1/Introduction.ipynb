{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Week 1: Introduction\n",
    "\n",
    "Instructor: Cornelia Ilin <br>\n",
    "Email: cilin@ischool.berkeley.edu <br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook is intended to introduce you to running ipython notebook and to familiarize you with some basics of numpy, matplotlib, and sklearn, which you'll use extensively in this course. \n",
    "\n",
    "Read through the commands, try making changes, and make sure you understand how the plots below are generated.\n",
    "\n",
    "In your projects, you should focus on making your code as organized and readable as possible. Use lots of comments -- see the code below -- !!\n",
    "\n",
    "You should also familiarize yourself with the various keyboard shortcuts for moving between cells and running cells. Ctrl-ENTER runs a cell, while shift-ENTER runs a cell and advances focus to the next cell."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Documentation\n",
    "\n",
    "[1] Generate evenly spaced numbers: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html\n",
    "\n",
    "[2] Python's lambda syntax: <br>\n",
    "http://www.python-course.eu/lambda.php\n",
    "\n",
    "[3] Return a sample from the standard Normal distribution: <br>\n",
    "https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.randn.html#numpy.random.randn\n",
    "\n",
    "[4] Sklearn documentation for linear regression: <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "[5] Sklearn documentation for the PolynomialFeatures preprocessor: <br>\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Define classes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Define functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Import packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# standard \n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# prediction\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# This tells matplolib not to try opening a new window for each plot\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Set working directories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5: Define global variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set a randomizer seeds (this will ensure the results are the same each time)\n",
    "np.random.seed(100)\n",
    "\n",
    "# set len(X)\n",
    "len_X = 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 6: Read data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This time we will generate our own data (y, X), by using a random number generator."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate X:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate evenly spaced X values in [0, 1]. Set len(X) = 20\n",
    "X = np.linspace(0, 1, len_X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Question 1: What is the data type of X?\")\n",
    "print(type(X))\n",
    "print(X)\n",
    "print (X.shape) # 1D ndarray "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "generate y:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a \"true\" function (a piece of a cosine curve) that we will try to approximate with a model\n",
    "true_function = lambda x: np.cos(1.5 * np.pi * x)\n",
    "\n",
    "# try this function out. Notice that you can apply it to a scalar, an array, or you can use it even in pandas\n",
    "print(true_function(0))\n",
    "print(true_function(0.5))\n",
    "print(true_function(np.array([0, 0.5])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate true y values\n",
    "y = true_function(X)\n",
    "\n",
    "# print the values of y to the nearest hundredth\n",
    "print (['%.2f' %i for i in y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# add random noise to y\n",
    "# the randn function samples random numbers from the standard Normal distribution\n",
    "# multiplying adjusts the standard deviation of the distribution\n",
    "noise = np.random.randn(len_X) * 0.2\n",
    "y += noise\n",
    "\n",
    "# print the noise-added values of y for comparison.\n",
    "print (['%.2f' %i for i in y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we want to predict y, using the feature vector X. \n",
    "\n",
    "In this course, our outputs (y) will always be 1-dimensional. Our inputs (X) will usually have more than 1 dimension. Today, for simplicity, we have just a single feature. \n",
    "\n",
    "Since the machine learning classes in sklearn expect input feature vectors, we need to turn each input x in X into a feature vector [x]."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 7: Clean data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# transform X into a 2D (in the numpy ndarray sense) vector\n",
    "X = X[:, np.newaxis]\n",
    "print(X)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 8: Analysis (model fit)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Linear model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model fit\n",
    "lm = LinearRegression(fit_intercept = True)\n",
    "lm.fit(X, y)\n",
    "lm_yhat = lm.predict(X)\n",
    "print(lm.intercept_)\n",
    "print(lm.coef_)\n",
    "print ('Estimated function: y = %.2f + %.2fx' %(lm.intercept_, lm.coef_[0]))\n",
    "print('Predicted values:')\n",
    "print(lm_yhat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Approximating a cosine function with a linear model doesn't work so well. By adding polynomial transformations of our feature(s), we can fit more complex functions. This is often called polynomial (nonlinear) regression. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Nonlinear model (poly degree==4)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create polinomial transformations\n",
    "poly = PolynomialFeatures(degree=4, include_bias=False)\n",
    "X4 = poly.fit_transform(X)\n",
    "print(X4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model fit\n",
    "lm4 = LinearRegression(fit_intercept=True)\n",
    "lm4.fit(X4, y)\n",
    "lm4_yhat = lm4.predict(X4)\n",
    "print(lm4.intercept_)\n",
    "print(lm4.coef_)\n",
    "print ('Estimated function: y = %.2f + %.2fx + %.2fx^2 + %.2fx^3 + %.2fx^4' %(lm4.intercept_, lm4.coef_[0], lm4.coef_[1], lm4.coef_[2], lm4.coef_[3]))\n",
    "print('Predicted values:')\n",
    "print(lm4_yhat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Nonlinear model (poly degree==15)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create polinomial transformations\n",
    "poly = PolynomialFeatures(degree=15, include_bias=False)\n",
    "X15 = poly.fit_transform(X)\n",
    "print(X15[0:3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model fit\n",
    "lm15 = LinearRegression(fit_intercept=True)\n",
    "lm15.fit(X15, y)\n",
    "lm15_yhat = lm15.predict(X15)\n",
    "print(lm15.intercept_)\n",
    "print(lm15.coef_)\n",
    "print('Predicted values:')\n",
    "print(lm15_yhat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 9: Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "degrees = [1, 4, 15]\n",
    "\n",
    "# Initialize a new plot and set plot size\n",
    "plt.figure(figsize=(14, 4)) \n",
    "\n",
    "for i in range(len(degrees)):\n",
    "    # create sublots that are all on the same row\n",
    "    ax = plt.subplot(1, len(degrees), i+1)\n",
    "    \n",
    "    # create the polynomial feature vector (or matrix)\n",
    "    poly = PolynomialFeatures(degree = degrees[i], include_bias = False)\n",
    "    temp_X = poly.fit_transform(X)\n",
    "    \n",
    "    # model fit\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(temp_X, y)\n",
    "    lm_yhat = lm.predict(temp_X)\n",
    "\n",
    "    # plot the true function\n",
    "    plt.plot(X, true_function(X), label=\"True function\");\n",
    "\n",
    "    # plot the true function with noise added\n",
    "    plt.scatter(X, y, label=\"Function with noise\");\n",
    "\n",
    "    # Show the fitted function for the linear model\n",
    "    plt.plot(X, lm_yhat, label=\"Fitted model\");\n",
    "\n",
    "    # Add labels, title, legend to the plot\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((-.05, 1.05))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree %d\" %degrees[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The machine learning lesson here is that we are interested in the smallest model that fits our data the best. Clearly, the degree 1 model, while very small (only 2 parameters), doesn't fit the observed data well. The degree 15 model fits the observed data extremely well, but is unlikely to generalize to new data. This is a case of \"over-fitting\", which often happens when we try to estimate too many parameters from just a few examples. The degree 4 model appears to be a good blend of small model size and good generalization."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "ac790038bef2eac1a9d9ccab13fd5e8972b5c1c9ef161fffafb9eb23a97c981f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}